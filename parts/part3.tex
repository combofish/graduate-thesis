
\BiChapter{基于焦点感知的光场显著性目标检测}{TODO}
\label{chap:part3}

\BiSection{研究动机}{TODO}
%
%
%
%
光场显着物体检测（LFSOD）由于光场中包含丰富的空间信息而引起了广泛的关注。
与 2D (RGB) 和 3D (RGBD) 数据不同，光场本质上捕获结构化 4D 表示，包括多视图图像、深度图和焦点切片。 其中，通过眼球运动顺序观察切片的焦点堆栈，
以及迎合人类视觉感知的可见注意力转移~\cite{piao2020dut}，做出适合显着性对象检测。

一些开创性的方法~\cite{zhang2019memory,piao2020exploit}~采用 ConvLSTM~\cite{shi2015convolutional}~，它使用记忆机制以预定义的顺序单独处理焦点堆栈特征。张等人~\cite{zhang2021learning}~后来在编码器阶段采用3D卷积来提取特征。刘等人~\cite{liu2021light}~和张等人~\cite{zhang2021geometry}~使用图神经网络来聚合不同焦点切片中的上下文信息。 这些方法依赖内存使用或大量计算来提取焦点堆栈特征，从而限制了效率。
我们重新思考光场数据建模的方式。考虑到焦点堆栈的成像效果，如图1所示，每个焦点切片根据空间透视深度的不同，聚焦部位也不同。 并且从同一场景生成，焦点切片有很多共同点。

因此，即使在很小的带宽内，也可以充分总结和传达它们之间的差异。 此外，给定图像，人类可以毫不费力地关注敏感部分并忽略不重要的背景。 因此，在焦点堆栈中处理更多相对的焦点切片来模拟人类视觉系统是合理的。 受上述观察的启发，我们考虑两个关键问题：

1）我们如何设计一个模型来存储切片级特征并在焦点堆栈和全焦点图像之间传递信息以进行上下文建模？ 
2）我们如何设计一个模型来理解场景的空间分布并感知敏感的焦点切片？ 
%
%
%
%
\par
在本文中，我们提出了一种用于高效且有效的 LFSOD 的焦点感知Transformer（FPT）。 具体来说，我们收集图像特定的特征并制定通信以加强焦点堆栈和全焦点图像之间的相互感知。 我们利用选择性机制将适当的焦点切片纳入检测。 具体来说，我们的贡献有三个：
%
%
%
%
\par
%
%
%
%
\begin{itemize}
	\item 我们引入与焦点相关的标记来总结图像特定的特征，并提出一种标记通信模块（TCM），通过计算与焦点相关的标记之间的交叉注意力来执行特征交互。 我们转移焦点堆栈中与焦点相关的标记以促进空间上下文传播。 
	
	\item 我们提出了一种焦点感知增强（FPE）策略，通过切片选择机制来增强焦点堆栈中的特征空间表示，以有区别地处理适当的焦点切片。	这可以突出显着切片并抑制非显着区域的干扰。 
	
	\item 我们对 4 个广泛使用的数据集进行了广泛的实验，并证明我们的方法优于现有最先进的 LFSOD 方法。 我们的方法在 DUTLF-FS~\cite{zhang2019memory}~上将 MAE 指标显着降低了 31\%。
\end{itemize}


 RBG显著性目标检测（SOD）~\cite{ ma2021pyramidal, wei2020f3net, zhou2020interactive}~、
 RGB-D~\cite{ cong2022cir, ji2021calibrated, liu2021visual}~和光场图像一直是活跃的研究领域。 
 在本文中，我们将主要研究LFSOD任务。
 现有的光场显着目标检测方法大致可以分为两类：（1）传统方法；（2）基于深度学习的方法。 传统方法通常采用手工制作的特征（例如，颜色对比度、纹理对比度和深度对比度）和先验（例如，位置先验、背景先验和边界连接先验）来检测显着对象。 
% 
%
%
%
 李等人~\cite{li2014saliency}~提出了第一个光场显着性数据集，并通过计算背景先验、位置先验和对比度线索来检测显着对象。
 之后，李等人~\cite{li2015weighted}~提出了加权稀疏编码框架同时处理2D、3D和4D SOD问题。
 张等人~\cite{zhang2015saliency}~计算对比度显着图，然后使用背景先验来消除背景干扰并获得最终结果。 
 张等人~\cite{zhang2017saliency}~基于随机搜索策略集成了从全焦点图像、深度图、焦点切片和多视图图像中提取的多个光场线索。 
 最近，Piao 等人~\cite{piao2019saliency}~提出了 LFSOD 的深度诱导元胞自动机。
 有关传统方法的更多详细信息可以在\cite{fu2022light}~中找到。
 到了深度学习时代，几种深度学习方法对光场SOD性能有了显着提升。 
 朴等人~\cite{piao2019deep}~首次尝试引入CNN来提取光场语义特征，并获得相应的显着图。 
 王等人~\cite{wang2019deep}~应用ConvLSTM~\cite{shi2015convolutional}~来融合CNN生成的特征，然后预测显着图。
 张等人~\cite{zhang2019memory}~还利用ConvLSTM来利用光场，并提出了用于显着性检测的最大光场数据集。 
 张等人~\cite{zhang2020lfnet}~利用提出的细化模块和集成模块的集成焦点堆栈特征来开发焦点切片。
朴等人~\cite{piao2020exploit}~提出了一种由焦点流和RGB流组成的不对称双流架构，以实现台式计算机和移动设备的多功能性。
%
%
%
%
\par
%
尽管大多数方法输入全焦点图像和焦点堆栈，但一些方法\cite{jing2021occlusion, wang2022lfbcnet, zhang2022exploring}~提出使用多视图和中心视图图像来检测显着对象。 
张等人~\cite{zhang2020light}~提出了一种深度网络，通过利用微透镜图像中丰富的角度信息来检测显着物体。 
张等人~\cite{zhang2021geometry}~提出了一种图神经网络，通过有效探索多视图图像之间的空间和视差相关性来预测显着图。 
静等人~\cite{jing2021occlusion}~提出了一种遮挡感知网络，从极平面图像（EPI）中提取遮挡边界特征以进行显着性检测。 
张等人~\cite{zhang2022exploring}~提出了一种光场合成网络来产生可靠的4D信息并驱动显着性检测。 
然而，上述方法的性能不如基于焦点堆栈输入的常见方法。 

这些使用焦点堆栈作为输入的方法集成了解码器中整个焦点堆栈的特征，忽略了不同切片对检测的相对贡献，并且容易受到非显着背景的影响。 因此，我们提出了更为适合 LFSOD 任务的设计。
 

Transformer，首先由 Vaswani 等人提出~\cite{vaswani2017attention}~，已广泛应用于自然语言处理（NLP）。 ViT 由 Dosovitskiy 等人提出。 [7]首先将Transformer应用于图像域。 由于其强大的全局信息捕获能力，变压器表现出了优异的性能。
最近的工作探索了将 Transformer 应用于各种视觉任务：图像分类 [3, 7]、对象检测 [60, 5, 35]、分割 [2, 41]、图像增强 [45, 2]、图像生成 [25] 和 视频处理 [59, 57]，以缓解 CNN 有限的全局信息学习能力。 此外，Transformer 已广泛应用于 RGB 显着目标检测 [20, 34] 和 RGBD 显着目标检测 [22, 40]，例如，Liu 等人。 [20]设计了一个基于纯变压器架构的统一模型，通过建模远程依赖性来预测显着性。 刘等人。 [22]提出了一种用于 RGB-D 显着目标检测的三元组变换器嵌入模块，通过学习跨层的远程依赖关系来增强高级特征。 塞里斯等人。 [34]提出了一种上下文实例转换器来捕获对象和场景上下文之间的上下文关系，以实现更准确的显着性推断。 王等人。 [40]提出了一种基于变压器的多模态融合模块来增强和融合RGB和深度图像特征。 受益于变压器的使用，这些方法可以获得更准确的场景上下文特征，并在复杂场景中表现出更好的检测性能。 然而，如何将变压器应用于 LFSOD 尚未得到全面探讨。 考虑变压器在建立长期依赖方面的优势，适合总结图像特定特征并通过附加标记传达信息。 在本文中，我们提出了一种令牌通信模块（TCM）来加强信息交互并促进上下文特征感知。


\BiSection{方法介绍}{TODO}

我们提出的 FPT 的整体架构如图 2 所示。给定 N + 1 个分辨率为 H × W 的图像，包括一个全焦点图像和 N 个焦点堆栈图像，我们将每个图像划分为补丁作为输入。 将展平的 { }N 个 patch 输入到线性投影中，得到嵌入的 patch fi 0 i=0，其形状为 (N + 1) × HW P 2 × C，其中 P 表示每个 patch 的大小，C 表示每个 patch 的大小。 渠道维度。 具体地，f0 0 表示全焦点图像的{ }N个块。 fi 0 i=1 表示焦点堆栈的斑块。 此外，为了总结补丁中的信息，引入了一组大小为 M × C 的随机初始化的可学习嵌入，作为 { }N 个焦点相关标记，表示为 m0 ，其中 M 表示焦点相关标记的数量。 我们将 i i=0 {[ ]}N 每个 patch 与焦点相关的标记连接起来，并得到 fi 0 , m0 i i=0 作为特征提取器的输入。 我们基于金字塔视觉变换器（PVT）[39]的主干网由 T = 4 个阶段组成，每个阶段包含 Ni ∈ [3,4,6,3] 变换器块。 如图 2 所示，每个 Transformer 块包括线性空间减少注意力 (SRA) 和前馈网络 (FFN)，以每个图像的方式作用于联合标记：其中 l ∈ 1..T 表示阶段编号 。 每个变压器编码器后面都布置了令牌通信模块（TCM），用于特征通信。 { }N 特征提取后，我们可以从 Transformer 块的输出中得到四层特征 fi 1, fi 2 , fi 3 , fi 4 i=0。 然后，采用共享权重特征金字塔网络（FPN）[18]进行分层融合并得到 { }N 个金字塔特征图 Fi 1, Fi 2, Fi 3 , Fi 4 i=0：其中 U p 贡献 2× 上采样 操作时，CBR 捐赠一个卷积块，包括 Conv+ BN+ RELU 层。 对于解码器，提出的焦点感知增强（FPE）策略增强了焦点堆栈的特征表示。 增强的焦点堆栈和全焦点特征融合在特征融合模块（FFM）中。 最后，采用掩模解码器来生成显着图。

\BiSubsection{令牌交互模块}{TODO}

在以前的 LFSOD 方法中，主干网络仅以原像方式执行特征提取 [31, 21]，忽略了光场数据固有的丰富上下文信息。 相比之下，我们设计了一个令牌通信模块（TCM）来对全焦点和焦点堆栈进行上下文建模，如图 3 所示。TCM 由令牌交互（TI）和令牌移位（TS）操作组成。 TC 计算机在全焦点和焦点堆栈的焦点相关令牌之间进行交叉注意，以进行信息交互。 TS 转移焦点堆栈中与焦点相关的标记，以促进上下文特征感知。 代币互动。 如图 3 (a) 所示，TC 由交叉注意力 (CA) 块组成。 注意力函数可以描述为将查询和一组键值对映射到计算为值的加权和的输出。 分配给每个值的权重是查询与相应键之间的相似度。 这里，我们采用带有残差连接和层归一化的缩放点积注意力来实现 CA 块，其可以表述如下： √ 其中 Q ∈ RNq ×dk 、 K ∈ RNk ×dk 和 V ∈ RNv ×dk 是 分别是查询、键和值。 dk是查询和密钥的通道维度，dk是控制softmax分布的温度参数。 对于CA块，K和V是相同的。 { }N { }N CA 块将所有与焦点相关的标记 m1 i i=0 作为输入。 焦点堆栈流的标记mi 1 i=1用作查询来计算与属于全焦点流的键m10的相似度并从值m10检索焦点信息。 该 CA 块的输出 Q1 可计算如下：其中 Q = m1 i i=1 W Q 、K = m10 W K 、V = m10W V 。 Q、K、V 和 Qlout 的维度分别为 N × M × C、M × C、M × C 和 N × M × C。 令牌移位。 上述操作的输出 Q1 被馈送到令牌移位操作中，如图 3 (b) 所示。 与焦点相关的标记被分为 G = 2 组，并沿着焦点深度轴以不同方向（向前或向后）循环移动。 通过不同的焦点深度和方向，令牌可以实现与近深度和远深度焦点图像的空间{}N上下文交换。 然后我们用连接的 m10 更新所有与焦点相关的标记 m1i i=0 并将 Q1 移出。 这种设计的目的是随着网络的深入，保持稳定的空间感受野。 值得一提的是，TS几乎是无参数的，带来的计算成本可以忽略不计。


\BiSubsection{聚焦感知增强策略}{TODO}


受人类视觉注意系统中关注选择性的启发，我们的目标是从多焦点特征中有效地感知有用的显着性信息。 我们提出了一种焦点感知增强（FPE）策略来模仿人类如何从视觉资源中选择感兴趣信息的筛选阶段。 FPE 由切片选择机制组成，用于有区别地处理相关焦点切片，如图 2 所示。选择机制可以突出显着切片并抑制非显着区域的干扰。 此外，我们采用结构相似性[42]来评估焦点切片和全焦点图像之间焦点的一致性，因为散焦状态下的物体不具有清晰的纹理结构。 首先，我们使用基于 FPN 的辅助解码器（AD）生成辅助显着性预测 P A 并应用监督以避免非显着对象的干扰并确保全焦点特征的有效性。 然后，我们使用全焦点显着性预测 P A 计算焦点堆栈 Fi l i=1 的每个层特征的结构相似度得分。 其中 CBR 捐赠了一个卷积块，它将特征压缩到一个通道。 SSIM表示结构相似度，可以表示为： 其中x和y捐赠两个输入图像，μx、μy和σx，σy分别是x和y的均值和标准差，σxy是它们的协方差，C1 = 0.012并且 C2 = 0.032 用于避免被零除。 生成scoreli后，我们选择前K个对应的特征作为增强数据：
其中T opK 是一种选择机制，在不改变原始空间顺序的情况下，根据scoreli 选择K 个最大值。 这种选择性焦点感知增强策略强调显着特征，同时抑制不必要的特征，这对于准确的显着目标检测至关重要。


\BiSubsection{训练过程}{TODO}
\todo 

在获得增强的焦点堆栈和全焦点特征后，逐步设计特征融合模块（FFM）来融合特征，如图2所示。与全焦点特征相比，焦点堆栈特征通常具有更高的数据维度， 一般为12倍大。 因此，平衡差异化的数据维度可以被认为是特征函数的前提任务。
一些简单的解决方案直接连接高维特征并使用 2D 卷积来压缩特征 [27] 或对每个焦点切片特征采用元素方式添加到融合数据 [21]。 然而，这些方法可能会阻止它们完全提取空间上下文信息，因为焦点堆栈在空间维度上是对齐的。 换句话说，生成的低维焦点堆栈特征无法提供足够的指导来实现高预测精度。 我们提出了一种基于 3D 卷积的通道压缩模块 (CCM) 来解决这个问题。 CCM引入了3D卷积，它可以本质上提取时空特征来融合所有焦点堆栈特征。
具体来说，对于形状为 k × C × h × w 的每一层增强焦点堆栈特征，我们将其重塑为 C × k × h × w，然后使用内核为 k × 1 × 1 的 3D 卷积来融合空间上下文特征 并压缩通道。 压缩特征C l 可以表示为： 其中CCM是3D卷积块，包括3D Conv+BN+RELU。 给定与输入形状相同的焦点堆栈和全焦点特征，采用多个卷积块进行跨域融合。 因此，我们将FFM表述如下：其中CBR 1和CBR2表示卷积块，包括Conv+BN+RELU，前者使用1×C通道输入，后者使用2×C。得到四层输出特征Ol后， 基于 FPN 的解码器 [18] 生成四个显着性 l=0 预测图。 最后一张用作最终的显着性预测图。 二元交叉熵（BCE）[6]是二元分类和分割中使用最广泛的损失函数。 但 Lbce 是像素级损失，这意味着它平等对待所有像素。

在具有主导背景的图片中，前景像素的损失将会被稀释。 最近，SOD[32]中引入了交集交集（IoU）来弥补 BCE 的不足。
Liou的目标是优化全局结构，而不是专注于单个像素，这样就不会受到分布不平衡的影响。 增强对齐度量[9]首先被提出作为一种可以同时考虑像素级和图像级误差的评估指标，损失形式为Lem = 1 − EΨ 。 基于上述讨论，我们构建了一个混合损失：如图 2 所示，我们的模型有两个掩码标头，它们预测辅助显着性图 PA 和 { }3 四个最终多尺度显着性图 Pi S i=0 。 因此，所提出的网络的总损失 Ltotal 可以表示为： 其中 G 表示地面实况显着性图。 L 代表我们用来逐渐优化预测的混合损失。 λ是控制辅助监督项权重的超参数。


我们的实验是在四个公共光场基准数据集上进行的：LFSD [17]、HFUT [49]、DUTLF-FS [51] 和 DUTLF-V2 [29]。 HFUT 和 LFSD 相对较小，分别仅包含 255 个和 100 个样本。 DUTLF-V2是最大的数据集，包含4204个样本，分为2957个和1247个分别用于训练和测试。 DUTLF-FS包含1462个样本，分别分为1000个训练样本和462个测试样本。 每个样本都包含一个全焦点图像、几个焦点切片以及相应的地面实况显着性图。

为了公平比较，我们遵循大多数以前的方法 [31, 21] 使用 DUTLF-FS 和 HFUT 的训练集来训练我们的 FPT，以便与使用焦点堆栈输入训练的其他光场方法进行比较。 我们按照 [37, 14] 使用 DUTLF-V2 的训练集来训练我们的模型，以便与使用多视图输入训练的其他最先进的 LFSOD 方法进行比较。 我们将每个图像的大小调整为 256 × 256，以便于在训练和测试中实现，并且我们还通过随机翻转、裁剪和旋转来增强训练数据。
我们使用预训练的 PVT-B2 [39] 模型作为我们的主干，因为它具有与 ResNet50 [12] 相似的计算复杂性。 我们共享全焦点和焦点堆栈模式之间的主干权重，以减少不必要的参数。 整个网络使用 Adam [15] 作为优化算法进行端到端训练，并将初始学习率设置为 1e-4。 小批量大小设置为 2，我们的网络训练了 80 个时期。 学习率在第 40 和 70 epoch 分别乘以 0.1。 所提出的方法是使用Pytorch工具箱[26]实现的，所有实验都在四个RTX 1080Ti GPU上进行。 我们的代码将可用。 定量比较：为了进行全面比较，我们将我们的方法与 20 个最先进的模型进行比较，包括 7 个 LFSOD 方法：DLGLRG [21]、RENet [31]、LFNet [50]、MAC [47]、MoLF [51]和DLSD[30]； 6种RGB-D SOD方法：DCF [13]、CIR-Net [4]、VST-rgbd [20]、BBS-Net [10]、SSF [52]和S2MA [19]； 和 7RGB 方法：VST-rgb [20]、PFSNet [23]、ITSD [58]、LDF [44]、MINet [24]、F3 Net [43] 和 EGNet [56]。 为了保证公平比较，我们使用他们提供的显着性预测图或预训练的权重来生成比较数据，并利用[20]提供的相同评估代码。 如表 1 所示，很明显，所提出的方法在 DUTLF-FS 和 HFUT 数据集上实现了比当前最先进的方法更优越的性能。 同时，所提出的方法可以在超过Sα的三个指标上超越其他方法。 值得一提的是，与使用大量数据集训练的 RGBD 和 RGB SOD 方法相比，该方法在小三倍的训练集（1100 vs. 2985 vs. 10553）下取得了显着的优势。 这表明我们的方法可以有效地探索光场数据中传达的信息。 我们还在 DUTLF-V2 上重新训练我们的方法，并与其他三种使用多视图图像作为输入的 LFSOD 方法进行比较，包括 OBGNet [14]、LFBCNet [37] 和 ESCNet [53]。 如表 2 所示，我们的方法可以在 DUTLF-V2 的所有四个指标上大幅实现最佳性能。 这证明了使用焦点堆栈作为输入的优越性。 定性比较：为了更直观地观察，图4中可视化了所提出的方法和其他排名靠前的方法生成的一些代表性结果。可以看出，我们提出的方法的结果与地面事实更加一致。 当面对这些具有挑战性的场景时，包括多个对象（第 4、5 行）和复杂场景（第 6,7 行），大多数 RGB 和 RGB-D SOD 方法无法准确检测显着对象。 相比之下，所提出的方法可以成功生成准确且稳健的显着图。 与这些基于 CNN 的 LFSOD 方法相比，所提出的方法获得了更一致的预测结果和更精细的细节。


\BiSection{实验结果与分析}{TODO}

\BiSubsection{实验设置}{TODO}
\BiSubsection{消融实验}{TODO}
\BiSubsection{对比实验}{TODO}

不同模型组件的有效性。 我们首先验证表 3 中不同模型组件的有效性。我们首先构建一个基线模型。 具体来说，它使用共享权重编码器来提取全焦点和焦点堆栈特征，然后直接连接它们并预测显着性图。 接下来，我们逐渐将我们提出的 TCM、FPE 和 CCM 采用到我们的 FPT 模型中。 这三个模型分别表示为“+TCM”、“++FPE”和“+++CCM”。 如表3所示，这三个模型可以逐渐提高LFSOD性能，最终大幅优于基线模型。 此外，为了证明我们TCM中TI和TS子模块的有效性，我们逐渐在“+TCM”模型中采用TI和TS，并将它们表示为“+TI”和“++TS”。 与“基线”相比。 “+TI”和“++TS”在 DUTLF-FS 上分别使 MAE 指标降低 29.2\% 和 43.8\%。 我们还在图5中提供了每个消融研究相应的可视化结果。可以发现，预测掩模与地面实况图变得更加一致。 显着对象的完整内容和精确的显着图边界证明我们的 FPT 模型可以过滤掉背景干扰并更多地关注显着对象。 焦点相关标记的数量。 在表 4 中，我们用焦点相关标记的数量从 8 个增加到 32 个来测试我们的方法。与较少的焦点相关标记 (M = 8, 16) 相比，更多的焦点相关标记 (M = 32) 可以获得更好的结果 。 除非另有说明，我们的实验均使用 32 个与焦点相关的标记进行。 焦点感知增强中的 K 值。 我们设置了多个比较实验来选择表 5 中的最佳 K 数。当我们将 K 从 1 增加到 5 时，性能逐渐提高。 当 K > 5 时，我们观察到模型性能饱和并变得次优。 我们认为较大的 K 值会引入更多不显着的背景效应。 考虑到性能和计算成本之间的权衡，我们选择配置 K = 5 作为最终配置。

在本文中，我们提出了一种用于精确光场显着物体检测的焦点感知变压器（FPT）。 我们引入与焦点相关的令牌来收集图像特定的特征，并提出令牌通信模块（TCM）来传播信息并促进空间上下文建模。 为了增强焦点堆栈的空间特征表示，我们提出了一种焦点感知增强（FPE）策略来帮助抑制噪声背景信息。 实验结果表明，我们提出的方法可以在大多数 LFSOD 数据集上实现最先进的性能。

\BiSection{本章小结}{TODO}