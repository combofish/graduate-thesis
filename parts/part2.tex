
\BiChapter{相关理论}{TODO}
\label{chap:part2}

\BiSection{光场技术基本理论}{TODO}

光场的概念最早由A.Gershun~\cite{gershun1939light}教授提出，是空间中光线集合的完备表示，采集并显示光场就能在视觉上重现真实世界。
1991年，MIT的Edward H.Adelson教授和James R.Bergen\cite{adelson1991plenoptic}教授指出人眼对光线的视觉感知可以认为是沿着单一函数的一个或多个方向的局部变化，描述了光照射到观察面的信息结构。
一旦定义了这个函数，各种潜在的视觉属性（如运动、颜色和方向）的测量就能够自动分离出来。
这个函数被称为全光函数，表示为：
%$$$$
\begin{equation}
	P(x,y,z,\theta,\varphi,\lambda,t)
\end{equation}\par
其中$(x,y,z)$为发光物体的空间位置，$(\theta,\varphi)$分别表示传播光线入射的垂直角度和水平角度，$\lambda$表示传播光线的波长，发光物体所发射的光线信息随时时间$t$的推移而变化。
然而，这种能够记录空间中光线信息的七维全光函数过于复杂、数据量大，难以记录和存储，在实际计算中并未得到应用。
需要对其进行简化处理。
McMillan等~\cite{mcmillan2023plenoptic}在七维全光函数的基础上提出了
简化波长$\lambda$和时间$t$的更为方便的五维光场模型。
\begin{equation}
	P(x,y,z,\theta,\varphi)
\end{equation}\par
五维光场模型通过记录红、绿、蓝三原色来简化波长$\lambda$，以及通过记录不同帧来简化时间$t$。
实际上，光线在空间传播中，因传播距离而造成的信息损耗微乎其微，光场模型还可以进一步简化。
%如果不考虑光线在空间传播过程中的衰减，记录光场信息的五维模型还可以进一步简化成四参数光场函数。
%\par
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.70\linewidth]{figures/chapter2/double-plane2}
	\bicaption{光场双平面四参数模型}{Light field biplane four-parameter model}  
	\label{chapter2_fig1:double_plane}
\end{figure}
Levoy等~\cite{levoy2023light}忽略掉传播距离维度$z$得到四维光场模型，
同时提出光场渲染理论和双平面模型来描述静态的可见光。
双平面模型利用两个互相平行的参数化平面表示四维光场。
假设光线在没有遮挡物和散射介质的区域，忽略光线在传播过程中波长和时间维度的变化，
则任意一个包含位置和方向信息的光线都可以用双平面参数来表示，
空间中的光线穿过这两个平面分别相交于点$(u, v)$和点$(s, t)$，
光线即可用四维光场函数表示，如图~\ref{chapter2_fig1:double_plane}~所示，其模型参数如下：
\begin{equation}
	P(u, v, x, y)
\end{equation}\par
在光场成像设备中，我们可以用$(u, v)$表示光线与微透镜阵列的交点，用$(s, t)$表示光线与CCD传感器探测面的交点。一条光线在整个四维空间中对应着光场的一个采样点。四维光场理论的推出为全光相机、相机阵列等设备提供了理论基础。目前，大多数单体全光相机和相机阵列的光场采集设备都遵循着四维光场理论。
%
%
%发展出了适用于光学系统的光场双平面参数特征。
%假设一条光线在两个不共面的平面$(u,v)$和平面$(s,t)$各有一个交点，则该光线可以用这两个交点唯一表示。
% \emph{et~al.}~
%光场是计算机科学领域的学者定义的“Light Field”，是指除了包含原图像矩阵中的空间坐标$(x,y)$和强度$I$外，还有光线入射的角度信息$(\theta,\varphi)$。
%光在传播过程中的各种潜在的视觉属性（如运动、颜色和方向）。


\BiSubsection{光场成像原理}{TODO}
在传统成像中，光线被捕获并呈现到成像平面上。然而，光场成像采用不同的方式。
它是一种计算成像技术，旨在捕获光线强度的同时记录光线的传播方向。
为了得到可视化的图像信息，必须对捕获的光场信息进行计算处理。
根据成像设备或记录方法的不同，现有的光场成像方式可以分为多传感器采集成像、
时间序列采集成像、空域复用成像和频域复用成像。\par
\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{figures/chapter2/camera_array}
	\bicaption{相机阵列光场成像}{Camera array for light field imaging}  
	\label{chapter2_fig2:camera_array}
\end{figure}
（1）使用相机阵列获取光场信息是通过多个相机以特定的空间分布在不同视角下捕获场景图像的方法，
如图~\ref{chapter2_fig2:camera_array}~所示。
每个相机捕获的是四维光场在其相对于场景方向上的二维投影。
通过融合这些相机捕获的图像，就可以获得完整的四维光场数据。
大尺度空间相机阵列主要应用于合成孔径成像以实现“透视”监测，或通过拼接实现大视角全景成像。
相比之下，紧密排列的相机阵列则主要用于捕获高性能动态场景或场景的三维分布和结构等信息。
光场数据的空间解析度取决于传感器本身，而角度解析度由传感器数量和布局方式确定。
这种采集方法能够在单次曝光中瞬时捕捉光场，因此还能记录光场的时间序列信息。
尽管这种成像方法空间解析度较高，但却带来了庞大的图像数据量，因此处理上更加耗时。
此外，这种成像方式对多传感器的相对位置要求也较高。\par
%
%
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/chapter2/time_seq2}
	\bicaption{时间序列光场成像}{TODO}  
	\label{chapter2_fig3:time_seq2}
\end{figure}
（2）除了多个相机阵列排布外，Marc Levoy等人\cite{levoy2023light}采用了单相机扫描系统。
他们通过让相机在固定的导轨上移动，分时获取不同空间视角下的场景图像，
最后将这些图像进行融合，从而实现相同的功能。
图~\ref{chapter2_fig3:time_seq2}~是典型的时序采集光场信息的示例。
与多传感器采集相比，这种方法的优势在于能够实现高密度角度分辨率的光场信息采集。
然而，缺点也显而易见，即需要高精度的控制，并且相对于多传感器采集，这种成像方式也更加耗时。
这些缺点限制了该成像方式仅适用于静态场景的光场信息采集。\par
%
%
%然而，由于需要进行扫描，这种方法更适用于静态场景的光场测量。
%此外，测量的精度受到相机移动定位精度的影响。
% 图3和图4展示了相机阵列以及单相机路径扫描方案实现光场信息获取的示例。
%
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.65\linewidth]{figures/chapter2/microlens_for_lf_imaging2.drawio}
	\bicaption{微透镜光场成像}{TODO}  
	\label{chapter2_fig4:microlens_for_lf_imaging}
\end{figure}
（3）空域复用的成像方式通过在图像传感器上安装微透镜阵列来实现，这是光场采集中常见的方式之一。
图~\ref{chapter2_fig4:microlens_for_lf_imaging}~展示了这种空域复用的成像方式的工作原理：通过将微透镜阵列插入普通成像系统的主透镜的一次像面，每个微透镜单元及其对应的传感器区域记录了光线从不同视角成像的集合，从而获得包含位置和传播方向的四维光场数据。
空域复用的成像方式优势在于结构紧凑，一次曝光即可捕捉光线强度与方向，适用于不同场景和动态场景的光场信息采集。
1992年，Adelson~\cite{adelson1992single}及其团队首次提出了利用微透镜阵列的光场相机模型；
然而，这种空域复用方式需要在空间分辨率和角度分辨率之间进行权衡。
当角度分辨率最大时，微透镜与图像传感器重合，空间分辨率最小；反之亦然。
为解决这一问题，
Georgiev~\cite{georgiev2010focused}在2021年提出了可调整微透镜与图像传感器相对位置的聚焦光场相机结构，
实现了角度分辨率与空间分辨率的动态调整。
%
% 相机阵列的体积庞大，限制了其应用范围。
% 通过缩小相机阵列中各成像单元之间的基线，可以在单个相机框架下利用微透镜阵列来采集光场信息。
% 空域复用的成像方式通过在图像传感器上安装微透镜阵列来实现，这是光场采集中常见的方式之一，可通过单次曝光捕获光场信息。
%
%
\BiSubsection{光场数据的可视化}{TODO}

根据先前的描述，四维模型中双平面光场模型是目前广泛采用的光场模型，其中两个维度表示空间位置，另外两个维度表示方向角度。
在三维世界中，描绘四维光场数据是具有挑战性的，可以通过固定双平面光场模型的任意两个维度来展示二维切片以可视化光场数据。
虽然二维光场切片无法传达完整的光场信息，但可以帮助解释光场数据的内在特征。
固定角度维度可以获得子孔径图像阵列形式的光场数据；固定空间维度可以获得宏像素形式的光场数据；固定一个角度维度和一个空间维度可以获得包含空间和角度信息的 EPI 形式的光场数据。焦点堆栈数据也是一种光场可视化的形式。
常用的光场可视化方式主要有子孔径图像阵列形式、宏像素形式、极线图形式和焦点堆栈。
\par
%
% %常用的光场可视化方式主要有多视角图像、极平面图像和焦点堆栈3种。
%
%
（1）
子孔径图像阵列视图是广泛使用的光场可视化方法之一。
固定角度维度$(u, v)$，令$u = u^{*}$，$ v = v^{*} $，
则$P(u^{*}, v^{*}, x, y) $表示在单个视点$(u^{*}, v^{*})$下的图像$(x,y)$。
若把$(x,y)$看成传统相机所成的像，$(u, v)$就表示相机所在的位置。
光场$P(u, v, x, y)$就可以理解为相机在视点平面等间隔采样，表现为相机不同视点$(u, v)$位置处所捕获的图像。
光场相机在不同视点下捕获的图像称为子孔径图像，又被称为亚光圈图像，简称为视图。
因此，四维光场数据可以被呈现为子孔径图像的阵列形式，如图~\ref{chapter2_fig5:multi_photo}~所展示。
图中右侧显示了以中心视点(5, 5)为基准的子孔径图像的放大示意图。\par
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/chapter2/multi_photo}
	\bicaption{子孔径图像阵列形式的光场数据可视化}{TODO}  
	\label{chapter2_fig5:multi_photo}
\end{figure}
%
%
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.80\linewidth]{figures/chapter2/macro_photo}
	\bicaption{宏像素形式的光场数据可视化}{TODO}  
	\label{cpt2_fig5:macro_photo}
\end{figure}
%
%
%multi_photo
%
%
（2）
宏像素视图是另一种光场可视化形式。固定空间维度的坐标$(x, y)$，令$x=x^{*}$，$y=y^{*}$，
则$ P(u, v, x^{*}, y^{*})$可以表示为单个宏像素，
通过按照空间分辨率遍历所有宏像素，并按照图像顺序排列，形成宏图像。
如图~\ref{cpt2_fig5:macro_photo}~所示。
每个宏像素在宏图像中的分辨率代表了光场的角度分辨率。
%
不考虑遮挡的情况，$ P(u, v, x^{*}, y^{*})$表示场景中某一物点在所有视点下对应的像素值，
即$ P(u, v, x^{*}, y^{*})$表示光场采集设备所捕获到的物点发出的所有光线，
如图~\ref{cpt2_fig5:macro_photo}~中标号\ding{193}、\ding{194}的宏像素所示。
考虑遮挡的情况，$ P(u, v, x^{*}, y^{*})$可由场景中的某一遮挡物点，
在可视点下对应的像点和在非可视点下其他物点对应的像点的组合，
在图~\ref{cpt2_fig5:macro_photo}~中标号\ding{192}、\ding{195}的宏像素中，
可以看到宏像素内含有不同物点发出的光线。\par
%
%
%
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/chapter2/epi_photos}
	\bicaption{极视角形式的光场数据可视化}{TODO}  
	\label{cpt2_fig6:epi_photos}
\end{figure}
%
%
%
（3）
极平面视图(Epipolar-Plane Images, EPI)是通过固定一个角度维度的坐标和一个空间维度的坐标得到的。
在多视角图像中，固定方向维度的角度域表示法主要用于展示光场的空间信息，
而固定空间维度的多视图表示法则着重于显示光场的方向信息。
相比之下，极平面图像通过固定一个角度坐标和一个空间坐标来展示场景的景深和结构信息。
具体来说，
当固定$(v, y)$坐标，令$v=v^{*}$，$y=y^{*}$，
则$ P(u, v^{*}, x, y^{*})$被称为固定$(v^{*}, y^{*})$的极平面图像，即垂直方向上的EPI图像。
同理，当固定$(u, x)$，则$ P(u^{*}, v, x^{*}, y)$被称为固定$(u^{*}, x^{*})$的极平面图像，
即水平方向上的EPI图像。
如图~\ref{cpt2_fig6:epi_photos}~所示，光场中同一水平视点上所有红色线条所在的行像素堆叠后就形成立下侧的EPI；
类似的，将光场中同一垂直视点上所有绿色线条所在的列像素堆叠后就形成立右侧的EPI。
当物体满足朗伯体表现的性质，即向各个方向发出的光线强度相等时，那么该点在极平面图像中被表示为一条直线，该直线的斜率可以反映出该点的深度信息。\par
%
%
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/chapter2/focal_stack}
	\bicaption{焦点堆栈形式的光场数据可视化}{TODO}  
	\label{cpt2_fig7:focal_stack}
\end{figure}
%
%
%
%
（4）
% 焦点堆栈数据是另一中常用的光场数据表示方法。
焦点堆栈数据是利用光场重新聚焦技术综合生成的，也是当前广泛应用的一种数据形式。
光场重新聚焦的核心是利用不同深度的物体在多视角下产生不同视差的原理，
通过平移叠加多视角图像来获得重新聚焦在特定深度的图像。
%
%
%
%
%
具体来说，
对于给定光场的多视角图像，选取某一行或某一列图像，确定特定深度，根据深度和视差的关系计算视差，
在叠加图像时按照该行或该列的顺序进行视差平移并叠加。
在这个过程中，与特定深度对应的物体在平移后与中心视角图像匹配像素，叠加后呈现清晰的聚焦像素，
而不在该深度范围内的物体在平移后匹配到不同像素，
叠加后呈现模糊的散焦像素。
通过选择一组深度，可以生成一组在不同深度聚焦的焦点堆栈数据。
%
%
%
在这些图像中，聚焦于物体深度的部分呈现清晰度，而非聚焦部分则显得模糊。
由于“近大远小”效应，不同图像之间存在放大比例差异。
放大比例记录了光线传播方向的信息，这些存在的放大比例可以进行光场的重投影。
这种特性使焦点堆栈数据包含丰富的场景结构信息，为显著性物体检测工作提供了重要支持。
\par
%
%
%
%\textcolor{red}{TODO}
%
%搜集聚焦堆栈数据的方法包括移动镜头或探测器，以采集在不同成像平面上聚焦的图像序列。在这些图像中，聚焦于物体深度的部分呈现清晰度，而非聚焦部分则显得模糊。由于“近大远小”效应，不同图像之间存在放大比例差异。放大比例记录了光线传播方向的信息，只有在存在放大比例时才能进行光场的重投影；而利用光场重聚焦技术（例如，位移和叠加）生成的重聚焦数据则不包含放大比例信息，能够用于深度重建，但无法重新投影出完整的光场。然而，对于深度重建来说，放大比例本身并无实际帮助。
%
%
%
%当固定x=x0, u=u0时，按照空间和方向的顺序排列在一起，就可以得到垂直方向的EPI图像；当固定y=y0, v=v0时，通过改变x的u的顺序，就能得到水平方向的EPI图像。当物体满足朗伯体表现的性质，即向各个方向发出的光线强度相等时，那么该点在极平面图像中被表示为一条直线，该直线的斜率可以反映出该点的深度信息。图2.6展示了一个极平面图像的例子。
%
%
%通过逐个选择所有单视角图像，便可以构建多角度视图。单个视角图像类似于RGB图像，可以呈现所拍摄场景的空间信息。这种固定方向参数、遍历所有可选单视角图像的方法称为多视角图像的多视图表示。当设定代表光场空间的两个维度为x = x0，y = y0时，可以得到单个宏像素。通过按照空间分辨率遍历所有宏像素，并按照图像顺序排列，形成宏图像。
%每个宏像素在宏图像中的分辨率代表了光场的角度分辨率，属于多角度视图的角度域表示法。图中展示的3x3角度分辨率和空间分辨率的多角度视图示例，其中(a)表示多视图表示法，(b)代表角度域表示法。
%
%
%
%
%
%当设定代表光场方向的两个维度参数为u = u0，v = v0时，就可以获得单个视角图像。
%
%
%
%
\BiSection{光场显著性目标检测相关理论}{TODO}
%
前文已指出，光场数据常用的四种表示方式，其中多视角图像和焦点堆栈数据常用于显著性目标检测。
本节首先探讨了基于这两种光场数据形式的显著性目标检测原理，并介绍了评估光场显著性目标检测的指标。
%
%
%前文已指出，光场数据常用三种方式表示，其中多视角图像和焦点堆栈数据常用于显著性目标检测。
%本节首先探讨了基于这两种光场数据形式的显著性目标检测原理，并最后介绍了评估光场显著性目标检测的指标。
%常用的光场可视化方式主要有子孔径图像阵列形式、宏像素形式、极线图形式和焦点堆栈。
%
%
\BiSubsection{基于多视角图像的显著性目标检测原理}{TODO}
%
%
基于多视角图像的显著性目标检测方法主要利用视差与深度之间的关系，在网络模型中引入了场景结构的线索。
通常，通过编码网络来提取多视角图像的高阶和低阶特征，并结合深度线索挖掘出的角度特征来定位显著性目标。
多视角图像的深度线索是通过不同视角之间的视差引入的。

具体来说，如图~\ref{cpt2_fig8:multi_array}~所示，
在观察同一目标$P$时，考虑两个视角 $v_{1}$ 和 $v_{2}$，
目标经过主透镜成像后形成像素点$P'$，
其中目标$P$到主透镜的距离为$Z$，而$Z'$表示主透镜到聚焦平面的距离，
主透镜到成像点的距离为$F$，主透镜到微透镜阵列的距离为$F'$。
两视点之间的距离用$H$表示，由此可推导出如下几何关系：
%
%
\begin{equation}
	\frac{D}{H} = \frac{F'}{Z'} - \frac{F'}{Z} 
	\label{cpt2_fac1:relate}
\end{equation}
%
%
其中$D$表示视差，$F'$、$H$和$Z'$都是相机内参，用来计算目标$P$点的深度。\par
%
%
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.80\linewidth]{figures/chapter2/multi_array}
	\bicaption{基于微透镜阵列的光场相机成像原理}{TODO}  
	\label{cpt2_fig8:multi_array}
\end{figure}
%
%
%
%
从公式~\ref{cpt2_fac1:relate}~可以看出，深度与多视角之间存在密切关系，根据这些关系建模是当前基于多视角图像的显著性目标检测的重要研究方向。

\textcolor{red}{TODO}




深度与多视角之间存在密切关系，根据这些关系建模是当前基于多视角图像的显著性目标检测的重要研究方向。一项具有代表性的工作是 MAC，该方法将微透镜图像阵列输入端到端的卷积神经网络中，首先对每个单视点图像的角度变化进行建模，然后将提取的特征输入到现有网络模型中，以捕获多尺度和长程空间依赖信息。此外，还有一些方法，如 DLSD，将光场显著性目标检测分解为两个子任务，合成多视角图像以定位显著性目标。通过利用视差与多视角图像的关系来完成合成光场的任务。


\BiSubsection{基于焦点堆栈的显著性目标检测原理}{TODO}
\BiSubsection{显著性目标检测性能评估指标}{TODO}


\BiSection{本章小节}{TODO}

本章首先阐述了光场技术的基本理论,介绍了光场的成像原理以及数据可视化形式;
然后介绍了光场显著性目标检测的相关理论,分别描述了基于多视角图像的显著性目标检测、
基于焦点堆栈的显著性目标检测方法的实现原理,
并引入了显著性目标检测中常用的几种性能评价指标。