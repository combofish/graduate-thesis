%%==================================================
%% chapter01.tex for DUT Thesis 
%% version: 0.1
%% last update: Dec 25th, 2022
%%==================================================




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\BiChapter{绪论}{Introduction}
%
%
%
%\BiSection{研究背景与意义}{Research Background}
%
%
%\BiSection{国内外相关研究工作进展}{Research Progress}
%\BiSubsection{RGB 显著性目标检测}{RGB Salient Object Detection}
%\BiSubsection{RGB-D 显著性目标检测}{RGB-D Salient Object Detection}
%\BiSubsection{光场显著性目标检测}{Light Field Salient Object Detection}
%
%
%\BiSection{论文主要内容及结构安排}{Main Content and Structural Arrangement}
%\BiSubsection{主要内容}{Main Content}
%\BiSubsection{结构安排}{Structural Arrangement}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiChapter{绪论}{Introduction}
%绪论应包括本研究课题的学术背景及其理论与实际意义；本领域的国内外研究进展及成果、存在不足或有待深入研究的问题；本研究课题的来源及主要研究内容等。

\label{chap:part1}
\BiSection{研究背景与意义}{Research Background}


随着信息技术的普及，社会已经进入多媒体时代。
信息以各种形式呈现，包括图文网站，
自媒体，短视频等。
图像作为生动的信息表达方式在现实世界中起着重要作用。
十年之前，人们通过昂贵的相机才能得到高质量的图片，
而如今，人手一个可以拍摄高清图片或视频的智能终端设备。
人们可获取的图像数据量急剧增长。
随之而来的是处理这些海量数据，
需要投入高额的人工时间。
研究学者设想可以利用电子设备来自动处理这些色彩丰富的数据。
由此逐渐形成了图像处理算法领域，以及后来的计算机视觉算法领域，
每年都会有大量研究者的探索。




图像并不只包含有效的信息，其本身存在冗余的背景信息或者噪声信息。
对图像中所有信息一视同仁的进行处理，
会造成计算资源的浪费。
因此，有高校研究者提出，
能否使计算机模仿人类视觉系统的处理过程，
聚焦于主要信息，从而减轻计算负担。
人眼视觉受控于大脑，能够辨识场景中引人入胜的目标物体，
然后进一步加工处理，这被称为人眼视觉的注意力机制。
这种机制使得网络能够快速捕捉到场景的重要内容，提高信息处理效率。
随着计算机5G通信技术的发展和手机、平板等智能终端设备的普及，
每个人都能随时随地的获取大量的图像视频等数据。
研究者探索让
电子计算设备能够模仿人类的视觉注意力的工作原理，
从而提高处理高维图像数据的效率，
促进有限计算资源的合理利用。
由此，
发展出了显著性目标检测任务，
通过仿照人类的视觉注意力机制，辨识图像或视频中显著的物体或区域。



%这一任务在计算机视觉、计算机图形学和机器人技术等多个领域发挥着重要作用，为其他视觉任务提供有效帮助，并在各种任务中应用广泛。

%显著性目标检测在计算机视觉、计算机图形学和机器人技术等领域扮演着关键角色。在计算机视觉领域中，显著性目标检测为其他视觉任务提供重要支持，
%在语义分割\upcite{li2014secrets}、
%目标检测\upcite{dai2016r}
%以及目标追踪\upcite{smeulders2013visual}
%等任务中有广泛应用。在计算机图形学领域，它被运用于
%自动图像裁剪\upcite{wang2018deep}、
%图像重定位\upcite{sun2011scale}和
%视频摘要\upcite{ma2002user}等任务。
%在机器人技术中，显著性目标检测被用于
%辅助人机交互\upcite{sugano2010calibration}和
%目标识别\upcite{karpathy2013object}等任务。



目前，显著性目标检测算法通常可以分为三类，
根据算法输入数据的形式，包括2D的彩色图像，
3D的彩色加深度图像，以及4D的光场图像。


\todo 

随着深度卷积神经网络的进步，显著性目标检测已经从传统手工特征转向主动探索图像语义特征，性能得到明显提升。基于深度学习的
2D显著性目标检测方法
\upcite{feng2019attentive,wu2019cascaded,wu2019mutual,liu2019simple,liu2018picanet,wang2018detect,hou2017deeply,liu2016dhsnet,wang2016saliency}
利用CNN挖掘RGB图像的特征，进而预测显著性图。
然而，在面对一些挑战性场景时，由于缺乏足够的场景信息支持，2D方法可能产生错误预测。


相较于2D数据，3D数据包含更多空间信息，并且随着3D传感技术的发展而更易获取。
基于深度学习的RGB-D显著性目标检测方法
\upcite{cong2019going,li2020asif,cong2017iterative,chen2019three,piao2019depth,chen2018progressively}
致力于融合深度特征和RGB特征，在推理显著图时考虑场景深度信息。许多RGB-D显著性目标检测方法因此获得较高检测性能，尤其在挑战性场景中表现突出。然而，3D方法受深度图质量影响，低质量深度图可能导致错误的预测，这是深度传感器采集问题带来的挑战。


相对于常规相机拍摄的RGB图像和RGB-D数据，光场相机捕捉的光场数据记录了更加全面和详尽的自然场景信息，包含深度线索、焦点线索和角度信息等。焦点堆栈数据是光场数据的一种表达方式，表示为一组在不同深度范围内聚焦的图像。
现有研究表明
\upcite{piao2019deep,zhang2020light,wang2019deep,zhang2019memory,zhang2020lfnet,piao2021panet}
，利用焦点堆栈数据的光场显著性目标检测方法在处理前景背景相似、复杂背景和小物体等具有挑战性场景时具有显著优势。然而，在实际场景中，光场数据获取成本高、处理多线索信息的成本高以及显著性像素级标注成本高，导致当前光场数据库样本数量较少。由于数据驱动的深度模型受数据限制，因此相同模型在训练数据较少的情况下会表现出较差的检测性能。因此，需要有效利用有限光场数据的网络模型，并通过增强当前光场数据集的算法来缓解这些问题。


因此，这篇文章将以有限数据驱动的光场显著性目标检测为研究重点，从网络模型和数据增强两个方面进行探索。首先，提出了区域感知网络，将焦点堆栈特征从全局整合到局部，更有效地利用现有光场数据。此外，还提出了一种基于数据增强的光场显著性目标检测方法，通过在重新组合显著性目标和背景的基础上合成新数据，从而扩充现有光场数据集。  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSection{国内外相关研究工作进展}{Research Progress}

近年来，显著性目标检测作为计算机视觉领域的重要基础任务之一，已经成为研究的焦点之一。
研究学者们提出了许多出色的检测模型，涵盖了
RGB 显著性目标检测（SOD）\upcite{ ma2021pyramidal, wei2020f3net, zhou2020interactive}、
RGB-D\upcite{ cong2022cir, ji2021calibrated, liu2021visual}和
光场显著性目标检测这三种类型。以下概述了近年来国内外该领域的研究进展。






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSubsection{RGB 显著性目标检测}{RGB Salient Object Detection}

目前的RGB显著性目标检测方法大致可以分为两类：传统方法和基于深度学习的方法。传统方法属于自底向上的方法，主要依赖各种颜色、纹理等低阶手工特征或对比度、中心环绕等启发式先验线索来识别显著性目标。这些传统方法的主要优势在于快速和适应性强，
其中常见的显著性模型包括对比度先验、中心先验和对象性先验。
对比度先验可分为局部对比度\upcite{itti1998model}和全局对比度方法\upcite{cheng2014global}，
前者主要通过像素或区域之间的对比度来定位显著性，而后者则考虑整个图像不同区域或像素之间的对比度。
中心先验假设显著性对象通常位于图像中心，可用于优化显著性预测图。
例如，张等人\upcite{zhang2015minimum}经过全局对比模型生成初始显著性图后，可以利用中心先验模型进一步提升图像质量。
Jiang等人\upcite{jiang2013salient}根据对象性先验改善粗略显著性图的性能，通过比较不同区域的特征来估计显著性图。


以上提及的方法利用确定性先验知识，在前景和背景对比度较高的场景中具有较高的检测准确性，但它们的泛化能力较差，对于不符合这些先验知识的场景，检测精度会明显下降。基于深度学习的方法属于自顶向下的方法，是一种数据驱动的方法，主要通过训练经过设计的网络模型来定位显著性目标，利用大规模带标签的数据库。由于卷积神经网络的发展，基于深度学习的检测方法的性能显著提高。一些方法集中在网络结构的设计上：
DSS\upcite{hou2017deeply}提出了多层特征密集连接的解码网络结构，结合不同尺度的特征以提高检测性能；
R3Net\upcite{deng2018r3net}提出了多层循环网络结构，通过迭代方式融合高阶和低阶特征，逐步优化预测结果；
CPD\upcite{wu2019cascaded}设计了部分解码网络结构以提高检测速度而不影响性能。随着注意力机制的发展，一些方法利用注意力机制挖掘场景中的有效特征。
例如，
PiCANet\upcite{liu2018picanet}结合了空间和通道注意力，将注意力模块嵌入多路循环网络以提高检测性能；
PAGRN\upcite{wang2018detect}设计了渐进式的注意力引导模块，有选择性地整合多层特征的上下文信息。
近年来，为了提高检测性能，一些显著性目标检测方法开始关注分割边界的性能，
如Luo等人\upcite{luo2017non}将IOU损失用作惩罚损失来提高预测图的边界性能；
另外，一些方法\upcite{zhuge2018boundary}在网络中明确地对互补的显著性目标信息和显著边缘信息进行建模，以维持显著目标边界的预测精度。






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSubsection{RGB-D 显著性目标检测}{RGB-D Salient Object Detection}


在RGB-D的显著性目标检测领域，深度图提供丰富的空间信息，这为在许多复杂场景下的显著性目标检测性能带来了显著提升。传统的RGB-D检测方法通常依赖形状、三维布局等低阶特征来定位显著性目标。
Peng等学者\upcite{peng2014rgbd}提出了多阶段的检测模型，结合深度信息和图像信息进行显著性预测。
Ren等研究者\upcite{ren2015exploiting}将RGB-D视为4通道数据来计算局部对比度，并将对比信息与全局先验知识相结合，提出了基于两阶段的显著性目标检测框架。一些RGB-D检测方法基于特定假设来定位显著性目标，比如认为靠近相机的目标更容易被识别为显著性目标。
Feng等学者\upcite{shigematsu2017learning}根据这一假设，提出了一种局部背景封闭特征来识别显著性目标。这些方法证明了深度信息在显著性目标检测任务中的重要作用。然而，与传统RGB检测方法类似，这些方法存在泛化性较差的问题，当不满足先验知识或假设时，检测精度会明显下降。随着深度学习的进展，卷积神经网络能够提取深度图和RGB图像的高阶语义特征，从而大幅提升了RGB-D检测方法的性能水平。


基于深度学习的 RGB-D 检测方法通常先提取RGB-D数据的特征，然后融合不同模态的特征来定位显著性目标，许多研究致力于研究更有效的跨模态特征融合方式。根据融合方式的不同，现有的基于深度学习的 RGB-D 方法可以大致分为前期融合、后期融合和多级融合三种方式。前期融合策略是指先融合多模态信息，然后提取特征来预测显著性，
Qu等研究者\upcite{qu2017rgbd}遵循这种方式，将RGB图像和深度图同时输入深度网络进行显著性预测。后期融合策略则是先提取多模态信息特征，然后融合这些特征来预测显著性，
Shigematsu等学者\upcite{shigematsu2017learning}采用后期融合方式，分别提取RGB图像和深度图的特征，然后将它们级联来定位显著性目标。与前述两种方式相比，多级特征融合在不同层级进行跨模态特征融合，使得不同层级的特征相互补充，在显著性目标检测任务中更为有效。
Chen等研究者在PCA中\upcite{chen2018progressively}采用多级特征融合方式，引入跨模态互补感知融合模块，考虑RGB和深度图之间的联系，在特征融合时获取更充分和有效的信息。
Piao等研究者\upcite{piao2019depth}提出了深度诱导的多尺度注意力网络，结合深度特征和RGB特征以不同尺度上下文信息融合，实现了精准的显著性定位。此外，研究者们还积极探索除特征融合外的RGB-D检测方法。
在CPFP中，Zhao等学者\upcite{zhao2019contrast}利用增强的深度信息来增强显著性目标与背景之间的对比度，并将其与RGB特征级联以预测显著性。
Chen等研究者\upcite{chen2019three}提出了一个自适应整合深度特征和RGB图像特征的注意力机制。






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSubsection{光场显著性目标检测}{Light Field Salient Object Detection}


在光场显著性目标检测领域，光场数据能够记录目标场景的空间信息并提供准确的深度信息，从而缓解困难场景下检测准确性受限的问题。光场数据记录了自然场景更全面、更完整的信息，对于显著性目标检测任务具有积极作用，因此越来越多关于利用光场数据提升检测性能的研究开始涌现。


现有的光场显著性目标标检测方法大致可以分为两类：（1）传统方法；（2）基于深度学习的方法。 传统方法通常采用手工制作的特征（例如，颜色对比度、纹理对比度和深度对比度）和先验（例如，位置先验、背景先验和边界连接先验）来检测显着对象。 
大多数方法采用先进行粗糙预测，然后在细化处理的多阶段检测方式，而且大部分研究都是基于光场焦点堆栈数据展开探索。
%
%
李等人\upcite{li2014saliency}提出了第一个光场显着性数据集，并通过计算背景先验、位置先验和对比度线索来检测显着对象。
之后，李等人\upcite{li2015weighted}提出了加权稀疏编码框架同时处理2D、3D和4D 显著性检测的问题。
利用非显著性字典对图像进行重构，并以高重构误差区域作为显著性字典，并通过迭代细化检测显著性目标。
张等人\upcite{zhang2015saliency}计算对比度显着图，然后通过计算深度线索对比度和RGB图像对比度生成显著性图，并利用背景先验增强显著性目标。
Wang等学者\upcite{wang2017two}提出了一个基于贝叶斯的显著性目标检测框架，能够有效整合从光场中提取的各种视觉特征。
张等研究者\upcite{zhang2017saliency}基于随机搜索策略集成了从全焦点图像、深度图、焦点切片和多视图图像中提取的多个光场线索。 
最近，Piao 等人\upcite{piao2019saliency}提出了
应用于光场显著性目标检测的深度诱导元胞自动机。
与RGB方法以及RGB-D方法类似，传统的光场检测方法由于手工特征的泛化性较差，很难推广到较为困难的场景。
有关传统方法的更多详细信息可以在\upcite{fu2022light}中找到。



到了深度学习时代，几种深度学习方法对光场SOD性能有了显着提升。 
基于深度学习的光场显著性目标检测方法可以根据输入数据分为两类：
一种是利用焦点堆栈数据的检测方法，另一种是利用多视角图像的检测方法。
大部分基于深度学习的光场方法都专注于利用焦点堆栈来检测显著性目标，致力于寻找更有效的多模态特征融合手段。
朴等人\upcite{piao2019deep}首次尝试引入卷积神经网络来提取光场语义特征，并获得相应的显着图。 
在DFS\upcite{wang2019deep}中，Wang等研究者利用ConvLSTM\upcite{chen2015convolutional}生成注意力向量，用于加权焦点切片特征和全聚焦图像特征，整合光场数据特征并预测显著性值。
Piao等团队\upcite{zhang2019memory}也运用ConvLSTM来探索光场数据，并创建了目前规模最大的光场显著性目标检测数据集。
在LFNet\upcite{zhang2020lfnet}中，Zhang等人提出了细化模块和整合模块，通过整合模块融合光场特征，并利用细化模块进一步优化显著性预测值。
在PANet\upcite{piao2021panet}中，Piao等研究者提出了一种区域级的探索光场数据方法，并提出了相应的特征整合策略，利用光场的全局信息来缓解大物体多目标检测不准确的问题。另一方面，利用多视角图像进行显著性检测的深度学习模型致力于从不同视角间挖掘有效特征之间的关联。
在DLSD\upcite{piao2019deep}中，Piao等学者将光场显著性目标检测拆分为两个子任务：通过合成多视角图像来检测显著性对象。
而Zhang等研究者\upcite{zhang2020light}在MAC中采用多视角图像阵列进行光场显著性目标检测，并建立了一个新的多视角图像阵列的数据集，通过对不同视角图像角度变化建模，
将提取的特征输入至DeepLabV2\upcite{chen2017deeplab}的结构中以捕获多尺度信息。
朴等人\upcite{piao2020exploit}提出了一种由焦点流和RGB流组成的不对称双流架构，以实现台式计算机和移动设备的多功能性。



尽管大多数方法输入全焦点图像和焦点堆栈，但一些方法\upcite{jing2021occlusion, wang2022lfbcnet, zhang2022exploring}提出使用多视图和中心视图图像来检测显着对象。 
张等人\upcite{zhang2020light}提出了一种深度网络，通过利用微透镜图像中丰富的角度信息来检测显着物体。 
张等人\upcite{zhang2021geometry}提出了一种图神经网络，通过有效探索多视图图像之间的空间和视差相关性来预测显着图。 
Jing等人\upcite{jing2021occlusion}提出了一种遮挡感知网络，
从极平面图像（Epipolar Plane Images，EPI）中提取遮挡边界特征以进行显着性检测。 
张等人\upcite{zhang2022exploring}提出了一种光场合成网络来产生可靠的4D信息并驱动显着性检测。 
然而，上述方法的性能不如基于焦点堆栈输入的常见方法。 
这些使用焦点堆栈作为输入的方法集成了解码器中整个焦点堆栈的特征，
忽略了不同切片对检测的相对贡献，并且容易受到非显着背景的影响。 





此外，Transformer 已广泛应用于 
RGB 显著性目标检测\upcite{liu2021visual, siris2021scene}和 
RGBD 显著性目标检测\upcite{liu2021tritransnet, wang2021mutualformer}
和光场显著性目标检测\upcite{liu2023lftransnet}。
Transformer 首先由 Vaswani 等人提出\upcite{vaswani2017attention}，
已广泛应用于自然语言处理（NLP）。
ViT 由 Dosovitskiy 等人提出\upcite{dosovitskiy2020image}，
首先将Transformer应用于图像域。
由于其强大的全局信息捕获能力，Transformer表现出了优异的性能。
最近的工作探索了将 Transformer 应用于各种视觉任务：
图像分类\upcite{chen2020generative, dosovitskiy2020image}、
对象检测\upcite{zhu2020deformable, dai2021up, sun2021rethinking}、
分割\upcite{chen2021pre, wang2021end}、
图像增强\upcite{yang2020learning, chen2021pre}、
图像生成\upcite{parmar2018image}和 
视频处理\upcite{zhou2018end, zheng2020end}，
以缓解 CNN 有限的全局信息学习能力。



也有不少研究学者将Transformer架构应用于显著性目标检测任务，如
刘等人\upcite{liu2021visual}设计了一个基于纯Transformer架构的统一模型，通过建模远程依赖性来预测显着性。
刘等人\upcite{liu2021tritransnet}提出了一种用于 RGB-D 显着目标检测的三元组变换器嵌入模块，通过学习跨层的远程依赖关系来增强高级特征。 
塞里斯等人\upcite{siris2021scene}提出了一种上下文实例转换器来捕获对象和场景上下文之间的上下文关系，以实现更准确的显着性推断。 
王等人\upcite{wang2021mutualformer}提出了一种基于Transformer的多模态融合模块来增强和融合RGB和深度图像特征。
受益于Transformer的使用，这些方法可以获得更准确的场景上下文特征，并在复杂场景中表现出更好的检测性能。 然而，如何将Transformer应用于光场显著性目标检测领域，
发挥Transformer架构在建立常成依赖方面的优势，
尚未得到全面探讨。 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSubsection{现存挑战}{Challenges}

由于深度学习的快速发展，网络模型能够提取出光场数据的高级语义特征，这导致光场显著性目标检测取得了显著的进展。对于跨模态特征融合，目前使用焦点堆栈数据的深度模型可以有效地融合多模态特征，并充分挖掘场景的几何关系。然而，深度学习方法是数据驱动的，因此用于光场显著性目标检测的深度模型性能受到数据量的影响。一般情况下，训练样本越多，模型的泛化能力就越好，检测效果也越强。不过，在实际场景中，获取光场数据需要多组摄像头或相机矩阵完成，这带来了极高的采集成本。而由于光场数据包含多个线索，如焦点堆栈、深度信息和多视角信息，处理这些数据的成本也很高。此外，为了进行显著性目标检测，需要对光场数据进行像素级标注，这需要耗费大量时间和精力。

这些问题导致现有光场显著性目标检测数据集非常稀缺，难以为现有深度学习算法提供足够的支持。
因此，需要有效利用有限的光场数据来设计网络模型，并采用能够对有限光场数据进行增强的算法来解决上述问题。然而，当前的光场显著性目标检测方法通常在网络模型设计上引入全局注意力模块，专注于关注深度范围内与显著性目标距离较近的焦点切片特征，却忽略了焦点堆栈中其他切片的特征，导致光场中大部分信息未得到充分利用。同时，现有的数据增强方法通常使用传统的方式，如图像翻转、旋转、裁剪和颜色变换等，这些方法基于先验知识，无法覆盖所有可能的测试场景。

综上所述，基于有限数据的光场显著性目标检测研究面临着挑战，并需要进一步深入研究。




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSection{论文主要内容及结构安排}{Main Content and Structural Arrangement}

\BiSubsection{主要内容}{Main Content}
%
%本篇文章以显著性目标检测为核心内容，着眼于探索受限数据驱动的光场显著性检测方法。研究从两个方面入手，一是设计能有效利用有限光场数据的网络模型，二是构建有效增强光场数据的算法，以缓解数据稀缺的挑战。在关于如何设计适用于有限光场数据的深度检测网络模型方面，本文提出了一种新的区域感知网络。这个网络与目前方法中使用的全局注意力不同，它从局部角度出发，考虑了每个焦点切片中不同区域对显著性预测的作用，更充分地利用了有限的光场数据。多源学习模块结合显著性、边界和中心位置信息生成特征整合策略，针对焦点堆栈的特征进行区域级整合，聚焦性识别模块考虑多聚焦特性对显著性的影响，并更新整合策略以更好地突出显著性区域并抑制非显著性区域。
%
%另一方面，关于如何设计有效增强光场数据的算法，本文提出了一种基于数据增强的光场显著性目标检测方法。相对于传统的数据增强方法，该方法引入了几何增强模块，通过结合图像修复网络和空间变换网络重新组合场景中的显著对象和背景，以尽可能扩增当前的光场数据集。聚焦性补偿模块则利用风格迁移网络进一步优化组合图像中焦点堆栈的真实性。此外，该方法还提出了一个不确定性学习策略，用于联合训练合成数据和真实数据，通过不同对待质量的合成数据，减小合成数据对网络训练的不利影响。



显著性目标检测的目标在于识别图像中最引人注目的对象或区域，这是计算机视觉领域中一个重要的任务。现有的显著性目标检测算法根据输入数据的类型可以被分为三类：RGB、RGB-D和光场方法。与RGB和RGB-D数据相比，光场数据包含了更丰富的场景信息，可以满足对于复杂场景信息的需求。近年来，随着深度卷积神经网络的发展，它取代了传统的基于手工特征的算法，显著提升了光场显著性目标检测的性能。
%
%
%然而，实际应用中存在较高的光场数据获取成本、复杂的光场多线索信息处理以及耗时耗力的显著性像素级标注，这导致当前光场显著性目标检测数据稀缺，为深度模型提供足够支持的数据不足。为解决这些问题，本文从高效利用光场信息和增广光场数据两个角度出发，探索利用有限数据驱动的光场显著性目标检测方法。
%
%
然而，实际应用中存在复杂的光场信息提取以及跨模态的光场信息融合难等问题，
这导致了当前光场显著性检测深度模型难以有效辨别光场场景的的显著性物体表示。
%
%
为了解决这些问题，本文从焦点感知和视角增强两个角度出发，
探索基于聚焦感知的光场显著性检测方法。
本文的主要工作及创新点如下所述：









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 第一个工作点
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
（1）
%
%
面对如何有效利用复杂场景中丰富的光场线索的挑战，
本文提出了一种聚焦感知网络探索光场数据的方法。
%
%
该方法主要包含两个模块：令牌通信模块和焦点感知增强模块。
%
%
其中令牌通信模块通过嵌入式令牌表示汇总建立全聚焦图片和焦点堆栈的切片级特征，
并通过令牌作为信息传递的桥梁，促进网络对空间上下文建模。
%
%
焦点感知增强模块充分考虑不同聚焦切片对于显著性的影响，
通过判断每个散焦切片的聚焦程度，来突出不同焦点切片中
显著性区域，同时抑制非显著性区域带来的负面影响。
%
%
相比现有的方法，本文方法通过附加嵌入式令牌的方式，
对光场的整体三维场景进行了切片级的探索，
并考虑了不同散焦切片对显著性预测的贡献，
能够更有效的利用光场信息。







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 第二个工作点
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
（2）
%
%
面对如何高效的利用光场数据中全聚焦图和焦点堆栈两个模态的差异信息，
本文提出了一种视角增强网络探索光场数据的方法。
%
%
该方法主要包含两个主要部分：视角增强注意力模块和感知对比学习策略。
%
%
其中视角增强注意力模块通过对两个模态做交叉注意力时引入跨模态的掩码表达，
加强了注意力权重在不同聚焦区域上的显著性表达。
%
%
感知对比学习策略考虑显著性预测的前景区域内部，与背景区域内部的一致性表达。
%
%
相比现有的光场显著性检测方法，本文方法对光场数据进行跨模态的特征融合，
充分考虑了焦点堆栈和全聚焦图对最终显著性预测的贡献，
能够产生更为鲁棒的显著性物体表达。



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\BiSubsection{结构安排}{Structural Arrangement}


第一章探讨了聚焦感知的光场显著性目标检测方法的背景和意义，
以及对国内外显著性目标检测领域的研究现状与进展作了介绍。
分析了目前方法中存在的问题与不足，并总结了本文的解决思路。


第二章，介绍在光场显著性检测任务重使用到的相关理论及方法。
主要包括光场技术的基本成像原理、光场数据不同的可视化表示方式、
基于多视角图像的光场显著性目标检测原理、
基于焦点堆栈的光场显著性目标检测原理
和显著性目标检测中用于评估模型性能的评价指标。


第三章详细介绍了基于焦点感知的光场显著性目标检测方法。
首先分析了现有光场显著性目标检测方法存在的问题，并阐述了研究的动机。
随后介绍了所提出的网络模型结构，
以及涉及到的令牌交互模块和焦点感知增强模块的具体应用方法。
最后，展示了实验设置和结果分析，证明了所提出方法的卓越性能。


在第四章中，详细阐述了基于视角增强的光场显著性目标检测方法。探讨了增强光场数据的研究动机，并详细介绍了几何增强模块、聚焦性补偿模块和不确定性学习策略的具体实施方法。最后，呈现了实验设置和结果分析，以验证所提出方法的显著优势。































